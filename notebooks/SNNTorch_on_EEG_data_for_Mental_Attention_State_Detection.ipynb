{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "0vN2UZv1H33A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def gpu_mem_state():\n",
    "  # Print out the GPU memory usage\n",
    "  print(\"Memory allocated:\", torch.cuda.memory_allocated() / 1024**3, \"GB\")\n",
    "  print(\"Max memory allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
    "\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkYUK1DAYYGC",
    "outputId": "e96fe856-2791-4bae-eb82-060281891e88"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 GB\n",
      "Max memory allocated: 0.0 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ResonatorSpikes:\n",
    "\n",
    "    def __init__(self, clk_freq, resonator_freq, spikes_path):\n",
    "        self.clk_freq = clk_freq\n",
    "        self.resonator_freq = resonator_freq\n",
    "        self.events = None\n",
    "        self._load_spikes(spikes_path)\n",
    "\n",
    "    def _load_spikes(self, spikes_path):\n",
    "        spikes_array = np.load(spikes_path)['spikes']\n",
    "        # if the file is already events based spikes\n",
    "        if np.max(spikes_array) > 1:\n",
    "            self.events = spikes_array\n",
    "        else:\n",
    "            self.events = np.where(spikes_array == 1)[0]\n",
    "\n",
    "    def spectrogram(self, window_ms):\n",
    "        window = int(self.clk_freq/1000 * window_ms)\n",
    "        N = self.events[-1] // window + 1\n",
    "        bins = np.zeros(N, dtype=int)\n",
    "        unique_indices, counts = np.unique(np.array(self.events) // window, return_counts=True)\n",
    "        bins[unique_indices] = counts\n",
    "        return bins\n",
    "\n",
    "\n",
    "class ChannelSpikes:\n",
    "\n",
    "    def __init__(self, base_folder, channel_name):\n",
    "        self.channel_name = channel_name\n",
    "        self.resonators_output = OrderedDict({})\n",
    "        self._load_resonators_output(base_folder)\n",
    "\n",
    "    def _load_resonators_output(self, base_folder):\n",
    "        channel_folder = base_folder / self.channel_name\n",
    "        for clk_freq in os.listdir(channel_folder):\n",
    "            clk_folder = channel_folder / clk_freq\n",
    "            for spikes in os.listdir(clk_folder):\n",
    "                resonator_freq = spikes[:-4]\n",
    "                self.resonators_output[resonator_freq] = ResonatorSpikes(int(clk_freq), float(resonator_freq), f'{clk_folder}/{spikes}')\n",
    "\n",
    "class SignalSpikes:\n",
    "\n",
    "    def __init__(self, signal_folder, label):\n",
    "        self.label = label\n",
    "        self.channels = OrderedDict({\n",
    "            channel: ChannelSpikes(signal_folder, channel)\n",
    "            for channel in os.listdir(signal_folder)\n",
    "        })\n",
    "\n",
    "\n",
    "class Trial:\n",
    "\n",
    "    def __init__(self, base_folder, trial):\n",
    "        self.trial = trial\n",
    "        self.base_folder = Path(f'{base_folder}/{trial}')\n",
    "\n",
    "    def load(self, minute):\n",
    "        # make sure it's in string format.\n",
    "        minute = str(minute)\n",
    "        for label in os.listdir(self.base_folder):\n",
    "            for m in os.listdir(self.base_folder / label):\n",
    "                if m == minute:\n",
    "                    return SignalSpikes(self.base_folder / label / m, label=label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "class EEGMentalSpikesDataset(Dataset):\n",
    "    def __init__(self, trials: List[Trial], minutes: List[int], time_sample_s: float, labels_mapper: Dict[str, int], spikes_buffer_size=50_000):\n",
    "        self.spikes_buffer_size = spikes_buffer_size\n",
    "        self.time_sample_s = time_sample_s\n",
    "        self.labels_mapper = labels_mapper\n",
    "\n",
    "        self.samples_per_minute = int(60 / time_sample_s)\n",
    "        self.samples_per_trial = self.samples_per_minute * len(minutes)\n",
    "        self.length = len(trials) * self.samples_per_trial\n",
    "\n",
    "        self.loaded_spikes = {\n",
    "            f'{i}-{j}': trial.load(minute)\n",
    "            for i, trial in enumerate(trials)\n",
    "            for j, minute in enumerate(minutes)\n",
    "        }\n",
    "        # get resonators clk frequencies.\n",
    "        signal4example = next(iter(self.loaded_spikes.values()))\n",
    "        resonators = next(iter(signal4example.channels.values())).resonators_output\n",
    "\n",
    "        self.map_channel_to_id = {ch: i for i, ch in enumerate(signal4example.channels.keys())}\n",
    "        self.map_resonator_to_id = {f: i for i, f in enumerate(resonators.keys())}\n",
    "\n",
    "        clk_freq = list(set(map(lambda x: x.clk_freq, resonators.values())))\n",
    "        # least common multiplier of the resonators is network clk frequency\n",
    "        self.network_clk = np.lcm.reduce(clk_freq)\n",
    "        self.max_buffer_size = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "\n",
    "        trial_id = id // self.samples_per_trial\n",
    "        minute_id = (id % self.samples_per_trial) // self.samples_per_minute\n",
    "        sample = ((id % self.samples_per_trial) % self.samples_per_minute)\n",
    "\n",
    "        spike_signal = self.loaded_spikes[f'{trial_id}-{minute_id}']\n",
    "\n",
    "        label = self.labels_mapper[spike_signal.label]\n",
    "\n",
    "        result = -np.ones((self.spikes_buffer_size, 2), dtype=np.int64)\n",
    "        result_index = 0\n",
    "\n",
    "        for ch, channel_spikes in spike_signal.channels.items():\n",
    "            ch_id = self.map_channel_to_id[ch]\n",
    "            for f, resonator in channel_spikes.resonators_output.items():\n",
    "                resonator_id = self.map_resonator_to_id[f]\n",
    "                ticks_in_sample = int(self.time_sample_s * resonator.clk_freq)\n",
    "                ts_spikes = resonator.events\n",
    "                start_var = sample * ticks_in_sample\n",
    "                end_var = (sample + 1) * ticks_in_sample\n",
    "\n",
    "                start_idx = np.searchsorted(ts_spikes, start_var, side='left')\n",
    "                end_idx = np.searchsorted(ts_spikes, end_var, side='right')\n",
    "                ts_spikes = ts_spikes[start_idx:end_idx]\n",
    "\n",
    "                # make sure all spikes are aligned even though the spikes come from different clocks and different timestamp!\n",
    "                ts_spikes = ts_spikes - (sample * ticks_in_sample)\n",
    "                ts_spikes = ts_spikes * int(self.network_clk // resonator.clk_freq)\n",
    "\n",
    "                neuron_id = ch_id * len(self.map_resonator_to_id) + resonator_id\n",
    "                try:\n",
    "                    result[result_index:result_index + len(ts_spikes), 0] = ts_spikes\n",
    "                    result[result_index:result_index + len(ts_spikes), 1] = neuron_id\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Number of spikes per sample is bigger than {self.spikes_buffer_size}\")\n",
    "                result_index += len(ts_spikes)\n",
    "\n",
    "        self.max_buffer_size = max(self.max_buffer_size, result_index)\n",
    "        return result, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "trial = Trial(f'../datasets/EEG_data_for_Mental_Attention_State_Detection/EEG_spikes_clk/', 10)\n",
    "\n",
    "labels_mapper = {\n",
    "    'drowsed': 0,\n",
    "    'focus': 1,\n",
    "    'unfocus': 2,\n",
    "}\n",
    "train_dataset = EEGMentalSpikesDataset(trials=[trial], minutes=[5, 15, 25], time_sample_s=.01, labels_mapper=labels_mapper, spikes_buffer_size=1_010_000)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "val_dataset = EEGMentalSpikesDataset(trials=[trial], minutes=[6, 16, 26], time_sample_s=.01, labels_mapper=labels_mapper, spikes_buffer_size=1_010_000)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1810011863708496"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "# for i in train_dataloader:\n",
    "#     continue\n",
    "# for i in val_dataloader:\n",
    "#     continue\n",
    "x, _ = next(iter(train_dataloader))\n",
    "time.time() - st\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "(10051, 0)"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.max_buffer_size, val_dataset.max_buffer_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([10, 1010000, 2]), Batch size 10, timestamp spikes 1010000 via all neurons\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape: {x.shape}, Batch size {x.shape[0]}, timestamp spikes {x.shape[1]} via all neurons')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 350)"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create B x vectors of size 350. for timestamp 100. every neuron that emit spike. put there 1.\n",
    "\n",
    "y = np.zeros((x.shape[0], 25*14))\n",
    "indices = (x[:, :, 0] == 100).nonzero()\n",
    "features = x[indices[:, 0], indices[:, 1], 1]\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5efD9OHkvVU"
   },
   "source": [
    "## Define The Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "I8WkfQIneZat",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6eb4efeb-bc6c-4dc7-d9e4-66ec56617f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 GB\n",
      "Max memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Define Network\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, network_clk, sample_time_s, num_inputs, beta=0.975):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network_clk = network_clk\n",
    "        self.sample_time_s = sample_time_s\n",
    "        self.steps = int(self.network_clk * self.sample_time_s)\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.fc1 = nn.Linear(num_inputs, num_inputs)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_inputs, len(labels_mapper))\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def event_ts_to_spikes(self, events_ts, t):\n",
    "        x = np.zeros((events_ts.shape[0], self.num_inputs))\n",
    "        indices = (events_ts[:, :, 0] == t).nonzero()\n",
    "        features = events_ts[indices[:, 0], indices[:, 1], 1]\n",
    "        x[indices[:, 0], features] = 1\n",
    "        return torch.tensor(x, requires_grad=True).float()\n",
    "\n",
    "    def forward(self, events_ts):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif1.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk_rec = [None] * self.steps\n",
    "        mem_rec = [None] * self.steps\n",
    "        for i in range(self.steps):\n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "            spikes = self.event_ts_to_spikes(events_ts, i)\n",
    "            spikes = spikes.to(device)\n",
    "            cur1 = self.fc1(spikes)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk_rec[i] = spk2\n",
    "            mem_rec[i] = mem2\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = SNN(train_dataset.network_clk, sample_time_s=.01, num_inputs=14*25).to(device)\n",
    "\n",
    "gpu_mem_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input time 97.27376389503479s\n",
      "step 43.07187747955322s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss = SF.mse_count_loss(correct_rate=.1, incorrect_rate=0.0)\n",
    "\n",
    "data, targets = next(iter(train_dataloader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "# targets = F.one_hot(targets, num_classes=len(labels_mapper))\n",
    "t0 = time.time()\n",
    "# forward pass\n",
    "net.train()\n",
    "spk_rec, mem_rec = net(data)\n",
    "t1 = time.time()\n",
    "print(f'input time {t1 - t0}s')\n",
    "_, idx = spk_rec.sum(dim=0).max(1)\n",
    "\n",
    "loss_val = loss(spk_rec, targets)\n",
    "# Gradient calculation + weight update\n",
    "optimizer.zero_grad()\n",
    "loss_val.backward()\n",
    "optimizer.step()\n",
    "t2 = time.time()\n",
    "print(f'step {t2 - t1}s')\n",
    "\n",
    "train_acc = np.mean((targets == idx).detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6144, 10, 3])"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spk_rec.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Km_5-ekz72U"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9ZF1z0XzsZ9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "outputId": "11a82f41-d7c6-4deb-aa6d-b1be76ab977c",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    with tqdm(total=len(train_dataloader) + len(val_dataloader)) as pbar:\n",
    "        # Minibatch training loop\n",
    "        minibatch_counter = 0\n",
    "        for data, targets in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data)\n",
    "\n",
    "            _, idx = spk_rec.sum(dim=0).max(1)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_counter += 1\n",
    "            avg_batch_loss = sum(loss_hist) / minibatch_counter\n",
    "\n",
    "            train_acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "            pbar.set_postfix(train_loss=f\"{avg_batch_loss:.3e}\", val_acc=f'{f\"{train_acc:.3e}\"}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        minibatch_counter = 0\n",
    "        # Val set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data, targets in train_dataloader:\n",
    "                val_data = val_data.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "\n",
    "                # Val set forward pass\n",
    "                val_spk, val_mem = net(val_data)\n",
    "\n",
    "                _, idx = val_spk.sum(dim=0).max(1)\n",
    "                val_acc = np.mean((val_targets == idx).detach().cpu().numpy())\n",
    "\n",
    "                # Val set loss\n",
    "                val_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "                for step in range(net.steps):\n",
    "                    val_loss += loss(val_mem[step], val_targets)\n",
    "                val_loss_hist.append(val_loss.item())\n",
    "                minibatch_counter += 1\n",
    "                avg_batch_loss = sum(val_loss_hist) / minibatch_counter\n",
    "                pbar.set_postfix(val_loss=f\"{avg_batch_loss:.3e}\", val_acc=f'{f\"{val_acc:.3e}\"}')\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ANN"
   ],
   "metadata": {
    "id": "zSChQREpKCbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(14, 32, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 3))\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.fc1 = nn.Linear(179456, 1024)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.dtype)\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "cnet = CNN().to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnet.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "id": "hhtgrWTCopta",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ed5a1e0-0076-4d84-b38c-493cf0473d7d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory allocated: 0.6859326362609863 GB\n",
      "Max memory allocated: 0.6859326362609863 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# del data\n",
    "# del targets\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gpu_mem_state()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AxsuUleTneA",
    "outputId": "8688ecc6-d773-4231-ccdb-0aec30b913b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory allocated: 0.6859326362609863 GB\n",
      "Max memory allocated: 0.6859326362609863 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "# Define the train and validation loops\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for data, target in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * train_correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in  tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = 100. * val_correct / len(val_loader.dataset)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Train and validate the CNN\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss, train_acc = train(cnet, train_loader, optimizer, loss, device)\n",
    "    val_loss, val_acc = validate(cnet, val_loader, loss, device)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "An4-iHRsQSuE",
    "outputId": "c6853dd9-d92f-40f3-f0c8-aa45de8d4f96"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train Loss: 0.047621, Train Acc: 32.21%, Val Loss: 0.046404, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2: Train Loss: 0.045774, Train Acc: 34.48%, Val Loss: 0.046393, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3: Train Loss: 0.045769, Train Acc: 34.48%, Val Loss: 0.046397, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046395, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046392, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6: Train Loss: 0.045768, Train Acc: 34.48%, Val Loss: 0.046390, Val Acc: 34.48%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-c08d150d81f3>\u001B[0m in \u001B[0;36m<cell line: 38>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0mn_epochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_epochs\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m     \u001B[0mtrain_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m     \u001B[0mval_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Epoch {epoch}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-c08d150d81f3>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mtrain_correct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Training\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mleave\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1177\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1178\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1179\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1180\u001B[0m                 \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    632\u001B[0m                 \u001B[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 678\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    679\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    680\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-884cfb72a501>\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, id)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0mdata1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{fname}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m       \u001B[0mdata2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{trial}_{start_time_ms+3000}.npz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m       \u001B[0mdata2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{self.samples_folder}/{trial}_{start_time_ms+2999}.npz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'spikes'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mmagic\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMAGIC_PREFIX\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    242\u001B[0m                 \u001B[0mbytes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 243\u001B[0;31m                 return format.read_array(bytes,\n\u001B[0m\u001B[1;32m    244\u001B[0m                                          \u001B[0mallow_pickle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mallow_pickle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    245\u001B[0m                                          pickle_kwargs=self.pickle_kwargs)\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001B[0m in \u001B[0;36mread_array\u001B[0;34m(fp, allow_pickle, pickle_kwargs)\u001B[0m\n\u001B[1;32m    776\u001B[0m                     \u001B[0mread_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_read_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    777\u001B[0m                     \u001B[0mread_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mread_count\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitemsize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 778\u001B[0;31m                     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_read_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mread_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"array data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    779\u001B[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001B[1;32m    780\u001B[0m                                                              count=read_count)\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001B[0m in \u001B[0;36m_read_bytes\u001B[0;34m(fp, size, error_template)\u001B[0m\n\u001B[1;32m    905\u001B[0m         \u001B[0;31m# done about that.  note that regular files can't be non-blocking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    906\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 907\u001B[0;31m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    908\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    909\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/zipfile.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m    925\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_offset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    926\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_eof\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 927\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    928\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    929\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_readbuffer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/zipfile.py\u001B[0m in \u001B[0;36m_read1\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m   1001\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compress_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mZIP_DEFLATED\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1002\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMIN_READ_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1003\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_decompressor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecompress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1004\u001B[0m             self._eof = (self._decompressor.eof or\n\u001B[1;32m   1005\u001B[0m                          \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compress_left\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(cnet.state_dict(), '\"/content/drive/MyDrive/SNN-Thesis/cnn_trained_model.pth')"
   ],
   "metadata": {
    "id": "8yIeqKAtT7-S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "api_key = '8236F572-BB36-431D-A64C-3A21B2751024'\n",
    "\n",
    "symbol = 'BTC_USDT'.upper().replace('-', '_')\n",
    "trades = []\n",
    "endpoint = f'https://rest.coinapi.io/v1/trades/BINANCEFTS_PERP_{symbol}/history'\n",
    "params = {\n",
    "    'apikey': api_key,\n",
    "    'time_start': '2023-03-21T08:59:54+00:00',\n",
    "    'limit': 100,\n",
    "}\n",
    "response = requests.get(endpoint, params=params, headers={'Accept': 'application/json'})\n"
   ],
   "metadata": {
    "id": "aLxavaeFWIKf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYbg_gWoCuVj",
    "outputId": "4b31814f-64bf-42e1-9379-08b73bb262f3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Dcfgz2n9CwBL"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
