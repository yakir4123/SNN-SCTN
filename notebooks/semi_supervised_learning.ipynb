{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "from snn.layers import SCTNLayer\n",
    "from snn.spiking_network import SpikingNetwork\n",
    "from snn.spiking_neuron import create_SCTN, BINARY\n",
    "from scripts.rwcp_resonators import create_neuron_for_labeling\n",
    "from utils import neurons_labels, save_network_weights, load_network_weights\n",
    "from utils.save_model import save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def create_neuron_for_labeling(synapses_weights):\n",
    "    neuron = create_SCTN()\n",
    "    neuron.synapses_weights = synapses_weights\n",
    "    neuron.leakage_factor = np.random.randint(4) + 1\n",
    "    neuron.leakage_period = (np.random.randint(4) + 1) * 100\n",
    "    neuron.theta = -1e-4 / neuron.leakage_factor\n",
    "    neuron.threshold_pulse = 50 * len(synapses_weights)\n",
    "    neuron.activation_function = BINARY\n",
    "    return neuron\n",
    "\n",
    "def create_random_network(freqs, clk_freq, n_neurons):\n",
    "    # create network with n neurons as labels.\n",
    "    # The neurons are not learning yet.\n",
    "    network = SpikingNetwork(clk_freq)\n",
    "    labels_neurons = [\n",
    "        create_neuron_for_labeling(np.random.random(len(freqs)) * 5 + 5)\n",
    "        for _ in range(n_neurons)\n",
    "    ]\n",
    "    network.add_layer(SCTNLayer(labels_neurons))\n",
    "    for neuron in network.layers_neurons[-1].neurons:\n",
    "        network.log_out_spikes(neuron._id)\n",
    "    return network\n",
    "\n",
    "\n",
    "def train_test_files(label, train_ratio=.5, seed=42):\n",
    "    files_names = np.array(os.listdir(f\"../datasets/RWCP_spikes/{label}\"))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    shuffle = np.random.permutation(len(files_names))\n",
    "    files_names = files_names[shuffle]\n",
    "    train = files_names[:int(len(files_names) * train_ratio)]\n",
    "    test = files_names[int(len(files_names) * train_ratio):]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_signals(test: bool, labels, seed=42, train_ratio=.5, oversample=False) -> List[Tuple[str, str]]:\n",
    "\n",
    "    files_names = {\n",
    "        label: train_test_files(label, seed=seed, train_ratio=train_ratio)[1 if test else 0]\n",
    "        for label in labels\n",
    "    }\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    if oversample:\n",
    "        max_samples = max(map(len, files_names.values()))\n",
    "\n",
    "        def oversample(files):\n",
    "            extra_samples = max_samples - len(files)\n",
    "            choices = np.random.choice(len(files), extra_samples)\n",
    "            return np.concatenate([files, files[choices]])\n",
    "\n",
    "        files_names = {\n",
    "            label: oversample(files_names[label])\n",
    "            for label in labels\n",
    "        }\n",
    "\n",
    "    signals_files = [\n",
    "        f'{label}/{f}'\n",
    "        for label, files in files_names.items()\n",
    "        for f in files\n",
    "    ]\n",
    "    labels = [\n",
    "        label\n",
    "        for  label, files in files_names.items()\n",
    "        for _ in range(len(files))\n",
    "    ]\n",
    "\n",
    "    res = np.array(list(zip(signals_files, labels)))\n",
    "\n",
    "    shuffle = np.random.permutation(len(res))\n",
    "    return res[shuffle]\n",
    "\n",
    "\n",
    "def activate_stdp_to_same_label_neurons(network: SpikingNetwork,\n",
    "                                        label: str,\n",
    "                                        ltp,\n",
    "                                        ltd):\n",
    "    time_to_learn = 15e-3\n",
    "    tau = network.clk_freq * time_to_learn / 2\n",
    "\n",
    "    for neuron in network.layers_neurons[-1].neurons:\n",
    "        if neuron.label == label:\n",
    "            neuron.set_stdp(ltp, ltd, tau, clk_freq, 30, -20)\n",
    "\n",
    "\n",
    "def activate_stdp_to_different_label_neurons(network: SpikingNetwork,\n",
    "                                             label: str,\n",
    "                                             ltp,\n",
    "                                             ltd):\n",
    "    time_to_learn = 15e-3\n",
    "    tau = network.clk_freq * time_to_learn / 2\n",
    "\n",
    "    for neuron in network.layers_neurons[-1].neurons:\n",
    "        if neuron.label != label and neuron.label is not None:\n",
    "            neuron.set_stdp(ltp, ltd, tau, clk_freq, 30, 0)\n",
    "\n",
    "\n",
    "def load_spikes_data(file_name, freqs, length=None):\n",
    "    spikes = pd.DataFrame \\\n",
    "        .from_dict(dict(\n",
    "            np.load(f'..\\datasets\\RWCP_spikes\\\\{file_name}')\n",
    "        ))\n",
    "    columns = [f'f{f}' for f in freqs]\n",
    "    res = spikes[columns].to_numpy()\n",
    "    if length is None:\n",
    "        return res\n",
    "    last_start_point = len(res) - length\n",
    "    if last_start_point <= 0:\n",
    "        return res\n",
    "\n",
    "    start_point = np.random.randint(last_start_point)\n",
    "    return res[start_point:start_point+length]\n",
    "\n",
    "\n",
    "def tag_neuron_a_label(network: SpikingNetwork,\n",
    "                       post_spikes: np.ndarray,\n",
    "                       label: str,\n",
    "                       neurons_labels_counter: Dict[int, Dict[str, int]]):\n",
    "    post_spikes = post_spikes.copy()\n",
    "    arg_most_active_neuron = np.argmax(post_spikes)\n",
    "    if post_spikes[arg_most_active_neuron] == 0:\n",
    "        return False\n",
    "    most_active_neuron = network.layers_neurons[-1].neurons[arg_most_active_neuron]\n",
    "    neuron_labels_counter = neurons_labels_counter[most_active_neuron._id]\n",
    "    # print(f'{most_active_neuron._id}: {neuron_labels_counter} + {label}')\n",
    "    neuron_labels_counter[label] += 1\n",
    "    max_label = max(neuron_labels_counter, key=neuron_labels_counter.get)\n",
    "    ratio_of_max_label = neuron_labels_counter[max_label] / sum(neuron_labels_counter.values())\n",
    "    if most_active_neuron.label != max_label:\n",
    "        most_active_neuron.label = max_label\n",
    "        return True\n",
    "    return False\n",
    "    # count_labels = sum(neuron_labels_counter.values())\n",
    "    # print(f'{most_active_neuron._id} for {label}. with {ratio_of_max_label}')\n",
    "    # # print(f'{most_active_neuron._id} for {label}. count_labels: {count_labels} with {ratio_of_max_label}')\n",
    "    # if count_labels >= 3 and ratio_of_max_label >= 1/3:\n",
    "    # # if ratio_of_max_label >= 1/2:\n",
    "    #     if most_active_neuron.label != max_label:\n",
    "    #         most_active_neuron.label = max_label\n",
    "    #         return True\n",
    "    # else:\n",
    "    #     most_active_neuron.label = None\n",
    "    #\n",
    "    # return False\n",
    "\n",
    "def amplify_inactive_neurons(\n",
    "        network: SpikingNetwork,\n",
    "        label: str,\n",
    "):\n",
    "    for i, neuron in enumerate(network.layers_neurons[-1].neurons):\n",
    "        if neuron.label == label:\n",
    "            neuron.synapses_weights *= 1.025\n",
    "\n",
    "\n",
    "def learning_process(\n",
    "        network: SpikingNetwork,\n",
    "        train_signals: List[Tuple[str, str]],\n",
    "        neurons_encoder,\n",
    "        epochs: int,\n",
    "        singal_length\n",
    "):\n",
    "    labels = list(neurons_encoder.keys())\n",
    "    neurons_encoder[None] = '-'\n",
    "\n",
    "    neurons_label_counter = {\n",
    "        neuron._id: {\n",
    "            label: 0\n",
    "            for label in labels\n",
    "        }\n",
    "        for neuron in network.neurons\n",
    "    }\n",
    "\n",
    "    neurons_removed = 0\n",
    "    total_runs = epochs * len(train_signals)\n",
    "\n",
    "    leakage_factor_arr = np.array([neuron.leakage_factor for neuron in network.neurons])\n",
    "    with tqdm(total=total_runs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            permutation_audio_file_indices = np.random.permutation(len(train_signals))\n",
    "\n",
    "            sum_epoch_accuracy_sum = 0\n",
    "            max_epoch_accuracy_sum = 0\n",
    "            mean_epoch_accuracy_sum = 0\n",
    "            sum_epoch_accuracy_count = 0\n",
    "            max_epoch_accuracy_count = 0\n",
    "            mean_epoch_accuracy_count = 0\n",
    "\n",
    "            for i, (signal, label) in enumerate(train_signals[permutation_audio_file_indices]):\n",
    "                stdp_lt_decay = .965 ** epoch\n",
    "                activate_stdp_to_same_label_neurons(network, label,\n",
    "                                                    ltp=1e-3 * stdp_lt_decay,\n",
    "                                                    ltd=-8e-4 * stdp_lt_decay)\n",
    "                activate_stdp_to_different_label_neurons(network, label,\n",
    "                                                         ltp=-8e-5 * stdp_lt_decay,\n",
    "                                                         ltd=0)\n",
    "\n",
    "                spikes = load_spikes_data(signal, freqs, length=singal_length)\n",
    "                post_spikes = network.input_full_data_spikes(\n",
    "                    spikes,\n",
    "                    False       # don't stop_on_first_spike\n",
    "                )\n",
    "\n",
    "                # normalize iot with leakage factor\n",
    "\n",
    "                post_spikes = post_spikes / leakage_factor_arr\n",
    "                if tag_neuron_a_label(network, post_spikes, label, neurons_label_counter):\n",
    "                    # if new neuron got a label,\n",
    "                    if epoch < 0:#epochs/2:\n",
    "                        # ill add another new unlabeled neuron to learn only if it has enough samples\n",
    "                        # to learn.\n",
    "                        neuron = create_neuron_for_labeling(np.random.random(len(freqs)) * 10)\n",
    "                        network.add_neuron(neuron, 0)\n",
    "\n",
    "                        neurons_label_counter[neuron._id] = {\n",
    "                            label: 0\n",
    "                            for label in labels\n",
    "                        }\n",
    "                amplify_inactive_neurons(network, label)\n",
    "\n",
    "                # prepare for new input\n",
    "                network.reset_learning()\n",
    "                network.reset_input()\n",
    "\n",
    "                predicted_sum, prediction_spikes_sum = predict_label_by_sum(\n",
    "                    neurons_encoder.keys(),\n",
    "                    network.layers_neurons[-1].neurons,\n",
    "                    post_spikes\n",
    "                )\n",
    "\n",
    "                predicted_max, prediction_spikes_max = predict_label_by_max(\n",
    "                    network.layers_neurons[-1].neurons,\n",
    "                    post_spikes\n",
    "                )\n",
    "\n",
    "\n",
    "                predicted_mean, prediction_spikes_mean = predict_label_by_mean(\n",
    "                    neurons_encoder.keys(),\n",
    "                    network.layers_neurons[-1].neurons,\n",
    "                    post_spikes\n",
    "                )\n",
    "\n",
    "                sum_epoch_accuracy_count += 1\n",
    "                sum_epoch_accuracy_sum += label == predicted_sum\n",
    "                max_epoch_accuracy_count += 1\n",
    "                max_epoch_accuracy_sum += label == predicted_max\n",
    "                mean_epoch_accuracy_count += 1\n",
    "                mean_epoch_accuracy_sum += label == predicted_mean\n",
    "                pbar.set_description(\n",
    "                    f\"{sum_epoch_accuracy_sum/sum_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                    f\"{max_epoch_accuracy_sum/max_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                    f\"{mean_epoch_accuracy_sum/mean_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                    f\" {label} {'V' if label == predicted_sum or label == predicted_max else 'X'} {prediction_spikes_max}/{prediction_spikes_sum}  | n removed: {neurons_removed} | {neurons_labels(network.layers_neurons[-1].neurons[:len(post_spikes)], encoder=neurons_encoder, spikes=post_spikes)}\")\n",
    "                pbar.update()\n",
    "\n",
    "                # if there are neurons with similar weights or zero weights, remove them.\n",
    "                if i > 0 and i % 25 == 0:\n",
    "                    neurons_removed += network.remove_irrelevant_neurons(1) # threshold\n",
    "                    leakage_factor_arr = np.array([neuron.leakage_factor for neuron in network.neurons])\n",
    "                    amplify_inactive_neurons(network, None)\n",
    "\n",
    "            print(\n",
    "                f\"{sum_epoch_accuracy_sum/sum_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                f\"{max_epoch_accuracy_sum/max_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                    f\"{mean_epoch_accuracy_sum/mean_epoch_accuracy_count * 100:.1f}%|\"\n",
    "                f\" Total neurons {len(network.layers_neurons[-1].neurons)} | total removed {neurons_removed}\"\n",
    "            )\n",
    "\n",
    "            if epoch == 10:\n",
    "                # fishy way to remove unlabeld neurons\n",
    "                for neuron in network.neurons:\n",
    "                    if neuron.label is None:\n",
    "                        neuron.synapses_weights = np.zeros(shape=neuron.synapses_weights)\n",
    "                neurons_removed += network.remove_irrelevant_neurons(1) # threshold\n",
    "                leakage_factor_arr = np.array([neuron.leakage_factor for neuron in network.neurons])\n",
    "\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                save_model(network, path=f'networks/semi_supervised_learning_e{epoch}.pickle')\n",
    "\n",
    "def predict_label_by_sum(labels, neurons, post_spikes):\n",
    "    labels_counter = {label: 0 for label in labels}\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        if neuron.label is not None:\n",
    "            labels_counter[neuron.label] += post_spikes[i]\n",
    "    return max(labels_counter.items(), key=operator.itemgetter(1))[0], \\\n",
    "        dict(sorted(labels_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "def predict_label_by_max(neurons, post_spikes):\n",
    "    arg_most_active_neuron = np.argmax(post_spikes)\n",
    "    most_active_neuron = neurons[arg_most_active_neuron]\n",
    "    return most_active_neuron.label, post_spikes[arg_most_active_neuron]\n",
    "\n",
    "\n",
    "def predict_label_by_mean(labels, neurons, post_spikes):\n",
    "    labels_counter = {label: [0, 0] for label in labels}\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        if neuron.label is not None:\n",
    "            labels_counter[neuron.label][0] += post_spikes[i]\n",
    "            labels_counter[neuron.label][1] += 1\n",
    "\n",
    "    labels_mean = {\n",
    "        label: spikes_sum/neurons_count if neurons_count > 0 else 0\n",
    "        for label, [spikes_sum, neurons_count] in labels_counter.items()\n",
    "    }\n",
    "    return max(labels_mean.items(), key=operator.itemgetter(1))[0], \\\n",
    "        dict(sorted(labels_mean.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "def semi_supervised_learning(\n",
    "        freqs,\n",
    "        clk_freq,\n",
    "        unlabeled_neurons,\n",
    "        signal_length=None,\n",
    "        epochs=1,\n",
    "        train_ratio=.5):\n",
    "    neurons_encoder = {\n",
    "        # 'bottle2': 'üç∂',\n",
    "        'dice1': 'üé≤',\n",
    "        'cherry1': 'üå≥',\n",
    "        'metal05': 'üßá',\n",
    "    }\n",
    "    network = create_random_network(freqs, clk_freq, unlabeled_neurons)\n",
    "    train_signals_files = get_signals(test=False,\n",
    "                                      labels=neurons_encoder.keys(),\n",
    "                                      train_ratio=train_ratio,\n",
    "                                      oversample=True)\n",
    "    learning_process(\n",
    "        network,\n",
    "        train_signals_files,\n",
    "        neurons_encoder=neurons_encoder,\n",
    "        epochs=epochs,\n",
    "        singal_length=signal_length\n",
    "    )\n",
    "    save_model(network, path=f'networks/semi_supervised_learning.pickle')\n",
    "    return network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f10cc98a492841f5903b9837dcdf8720"
      },
      "application/json": {
       "n": 0,
       "total": 6000,
       "elapsed": 0.004000186920166016,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.7%|57.3%|48.0%| Total neurons 44 | total removed 6\n",
      "51.3%|52.0%|43.3%| Total neurons 43 | total removed 7\n",
      "53.3%|52.0%|44.7%| Total neurons 42 | total removed 8\n",
      "49.3%|48.0%|38.7%| Total neurons 42 | total removed 8\n",
      "48.0%|50.0%|46.0%| Total neurons 42 | total removed 8\n",
      "46.7%|48.7%|45.3%| Total neurons 42 | total removed 8\n",
      "46.7%|51.3%|46.0%| Total neurons 39 | total removed 11\n",
      "48.7%|51.3%|46.0%| Total neurons 38 | total removed 12\n",
      "44.7%|46.7%|43.3%| Total neurons 38 | total removed 12\n",
      "42.0%|44.0%|44.0%| Total neurons 38 | total removed 12\n",
      "42.0%|45.3%|44.0%| Total neurons 37 | total removed 13\n",
      "38.0%|37.3%|36.7%| Total neurons 36 | total removed 14\n",
      "41.3%|46.0%|42.7%| Total neurons 33 | total removed 17\n",
      "44.7%|45.3%|40.0%| Total neurons 33 | total removed 17\n",
      "40.7%|43.3%|42.0%| Total neurons 32 | total removed 18\n",
      "44.0%|44.7%|40.7%| Total neurons 32 | total removed 18\n",
      "43.3%|47.3%|40.0%| Total neurons 32 | total removed 18\n",
      "36.0%|43.3%|36.7%| Total neurons 32 | total removed 18\n",
      "38.7%|38.7%|36.7%| Total neurons 32 | total removed 18\n",
      "43.3%|45.3%|41.3%| Total neurons 32 | total removed 18\n",
      "38.0%|42.7%|42.0%| Total neurons 32 | total removed 18\n",
      "38.7%|41.3%|41.3%| Total neurons 32 | total removed 18\n",
      "37.3%|42.7%|40.0%| Total neurons 32 | total removed 18\n",
      "34.0%|44.7%|39.3%| Total neurons 32 | total removed 18\n",
      "38.0%|42.7%|42.0%| Total neurons 32 | total removed 18\n",
      "43.3%|44.0%|46.7%| Total neurons 31 | total removed 19\n",
      "38.0%|44.7%|41.3%| Total neurons 30 | total removed 20\n",
      "40.0%|44.0%|41.3%| Total neurons 30 | total removed 20\n",
      "42.7%|43.3%|46.0%| Total neurons 30 | total removed 20\n",
      "42.0%|43.3%|43.3%| Total neurons 29 | total removed 21\n",
      "42.7%|42.7%|44.7%| Total neurons 29 | total removed 21\n",
      "39.3%|45.3%|36.7%| Total neurons 28 | total removed 22\n",
      "41.3%|41.3%|42.0%| Total neurons 28 | total removed 22\n",
      "46.0%|42.7%|46.0%| Total neurons 28 | total removed 22\n",
      "43.3%|42.0%|43.3%| Total neurons 28 | total removed 22\n",
      "42.0%|38.7%|42.0%| Total neurons 28 | total removed 22\n",
      "45.3%|44.7%|44.7%| Total neurons 28 | total removed 22\n",
      "44.0%|41.3%|43.3%| Total neurons 28 | total removed 22\n",
      "45.3%|44.7%|46.0%| Total neurons 28 | total removed 22\n",
      "42.7%|42.0%|41.3%| Total neurons 28 | total removed 22\n"
     ]
    }
   ],
   "source": [
    "clk_freq = int(1.536 * (10 ** 6) * 2)\n",
    "\n",
    "freqs = [\n",
    "    # 236, 751, 887, 1046, 1235, 2029, 2825, 3934, 5478\n",
    "    200, 236, 278, 328, 387, 457,\n",
    "    751, 887, 1046, 1235, 1457,\n",
    "    1719, 2029, 2825, 3934, 5478\n",
    "]\n",
    "network = semi_supervised_learning(freqs,\n",
    "                                   clk_freq,\n",
    "                                   unlabeled_neurons=50,\n",
    "                                   # signal_length=clk_freq // 5,\n",
    "                                   epochs=40,\n",
    "                                   train_ratio=.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "153600"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clk_freq //20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.11086807, -0.80661419,  1.55629568, -2.5134114 ,  2.43022647,\n        3.1839897 , -5.07300176, -0.34431008,  2.78732933])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = network.layers_neurons[-1].neurons[-2]\n",
    "n.synapses_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 0.17418922701791142, s: 1.567703043161203\n",
      "m: 0.014220146044304747, s: 0.12798131439874272\n",
      "m: 0.09121399036537664, s: 0.8209259132883897\n",
      "m: 23.857278257783364, s: 214.7155043200503\n",
      "m: 3.216422223713165, s: 28.947800013418483\n",
      "m: 2.9157538741906257, s: 26.24178486771563\n",
      "m: 14.36104289820265, s: 129.24938608382385\n",
      "m: 0.11426193986906845, s: 1.028357458821616\n",
      "m: 0.1479302039393225, s: 1.3313718354539024\n",
      "m: 0.11139330782818216, s: 1.0025397704536394\n"
     ]
    }
   ],
   "source": [
    "for n in network.layers_neurons[-1].neurons:\n",
    "    # n = network.layers_neurons[-1].neurons[-2]\n",
    "    print(f'm: {np.mean(n.synapses_weights)}, s: {np.sum(n.synapses_weights)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.42634979, -0.9356769 ,  1.15019126, -2.01583432,  2.13429621,\n         3.19053626, -3.04678838, -1.93266906,  2.59729819],\n       [ 1.42604832, -1.43266243,  3.18843389,  1.38256309, -5.61179337,\n         2.89713318, -2.45024665, -2.66220082,  3.39070609],\n       [ 0.36689498,  0.13862808,  0.86473384, -1.25849823,  1.81880796,\n         2.95287966, -1.79905318, -4.44386731,  2.18040013],\n       [24.98881504, 24.9888165 , 24.99850451, 24.92681483, 24.94278241,\n        24.98002726, 14.90951792, 24.98330274, 24.99692312],\n       [ 1.20164648,  3.41879667,  0.91799066,  0.94156988,  3.11413309,\n         9.79510529,  1.7533027 ,  0.17161102,  7.63364423],\n       [ 2.88630553,  5.81238221,  1.54362715,  4.81140102,  5.32589433,\n         0.51823537,  3.36604278,  1.34414677,  0.6337497 ],\n       [14.16051573, 13.29360264, 14.61968991, 12.7890441 , 15.00243343,\n        15.38766173, 15.07142736, 13.97815128, 14.9468599 ],\n       [ 0.4811429 ,  0.10211204,  1.59119002, -3.48779611,  2.21098255,\n         3.52022334, -5.82102702, -0.31198255,  2.74351228],\n       [ 0.11086807, -0.80661419,  1.55629568, -2.5134114 ,  2.43022647,\n         3.1839897 , -5.07300176, -0.34431008,  2.78732933],\n       [ 0.2800909 , -0.02661971,  1.20451376, -1.8802232 ,  1.95377122,\n         3.00399172, -2.05294247, -3.48605864,  2.0060162 ]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([n.synapses_weights for n in network.layers_neurons[-1].neurons])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_process(network, test_signal_files):\n",
    "    labels = [f'{n.label}_{n._id}' for n in network.layers_neurons[-1].neurons]\n",
    "    predict_results = []\n",
    "    for signal_file, label in tqdm(test_signal_files):\n",
    "        spikes = load_spikes_data(signal_file, freqs)\n",
    "        post_spikes = network.input_full_data_spikes(spikes)\n",
    "        res = dict(zip(labels, post_spikes))\n",
    "        res['label'] = label\n",
    "        predict_results.append(res)\n",
    "        network.reset_input()\n",
    "\n",
    "    df = pd.DataFrame.from_records(predict_results)\n",
    "    df.to_csv('output_spikes/semi_supervised_test.csv', index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_network(network=None,\n",
    "                 freqs=None,\n",
    "                 clk_freq=None,\n",
    "                 n_neurons=None,\n",
    "                 train_ratio=.5):\n",
    "    if network is None:\n",
    "        network = create_random_network(freqs, clk_freq, n_neurons)\n",
    "        load_network_weights(network, path='neurons_weights/semi_supervised_learning.pickle')\n",
    "    test_signals_files = get_signals(test=True, train_ratio=train_ratio, oversample=False)\n",
    "    return test_process(network, test_signals_files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 4.35672899e+00,  7.30039317e+00,  4.77161277e-01,\n         5.66037210e+00,  1.58646448e+00,  1.20164648e+00,\n         3.41879667e+00,  9.17990658e-01,  9.41569883e-01],\n       [-1.26426331e+00, -3.72111760e+00,  1.18103599e+00,\n         1.64843097e+00,  3.70811759e+00, -2.46478648e+00,\n        -1.52717352e+00,  9.16547228e-02,  2.55114505e+00],\n       [ 1.87490623e+00, -1.20435093e+01,  5.01675772e-01,\n        -2.25845343e+00,  3.64779624e+00,  3.56363266e+00,\n        -1.55391389e+01,  2.44251172e+01, -4.00651120e+00],\n       [ 3.64591807e-03, -2.23123428e+00,  3.15930862e+00,\n        -2.73905917e-02, -8.15936886e-01, -3.59395366e-01,\n         1.97396357e-01, -7.46947894e-01,  1.41968974e+00],\n       [ 7.32743461e-01, -9.25002166e-02,  3.04672375e+00,\n        -1.09355767e+00,  1.19112157e+00,  3.02269213e+00,\n         5.90710433e-01, -4.85874360e+00, -2.29784524e+00],\n       [-4.62760390e+00, -1.70754592e+00,  1.27750317e+00,\n        -1.35984487e+00, -4.30918389e-02,  4.80076196e+00,\n        -1.44798551e+00,  3.49265410e-01,  3.39918451e+00],\n       [ 6.33400854e+00,  2.40145619e+00,  7.58633281e-01,\n         1.28879722e+00,  1.28045839e+00,  1.51902694e+00,\n         1.38827173e+00,  6.40874745e+00,  1.81880084e+00],\n       [-1.89186344e+00, -1.06707310e+01,  5.30092048e+00,\n        -6.58334704e+00,  3.99602510e+00,  6.16473382e+00,\n        -1.29655504e+01,  2.43991548e+01, -6.76757342e+00],\n       [ 2.09425341e+00, -2.00685021e+00,  3.98469758e+00,\n        -2.62770219e+00, -1.33131149e+00, -2.56183686e+00,\n        -5.10849948e+00,  6.30823281e+00,  2.71334279e+00],\n       [-3.73987368e+00, -6.08056815e+00, -1.23142249e+00,\n         2.65092198e+00, -1.28098860e+00, -1.92517973e+00,\n        -7.06526433e+00,  1.52841162e+01,  3.82267793e+00],\n       [ 4.14894301e+00,  7.19530017e+00,  4.95072572e+00,\n         5.17531412e+00,  4.74152396e+00,  7.49872984e+00,\n        -1.28451464e+01,  4.33253268e+00,  7.12433832e+00],\n       [-2.77442469e-01, -2.07460001e+00,  1.02716277e+00,\n        -2.09005833e+00,  2.07245049e+00,  3.05243697e+00,\n        -1.39385329e+00, -6.01400100e-01,  1.66295215e+00],\n       [-1.39842210e-01, -5.67603329e+00,  3.02328803e-01,\n        -7.74044376e-01, -3.37207750e+00, -1.55076226e+00,\n        -9.72392631e+00,  2.11014292e+01,  6.03142070e-01],\n       [-5.27037871e+00, -3.39990873e+00,  5.07481817e+00,\n        -3.90343556e+00,  2.12481050e+00,  5.02268719e+00,\n        -9.34327197e+00,  1.54730171e+01, -3.20594909e+00],\n       [-4.17735537e+00, -4.37475679e+00,  4.31549999e+00,\n         3.53524221e-01, -2.56555328e+00,  6.25251750e-01,\n        -4.23870713e+00,  1.10151880e+01,  4.19635212e-01],\n       [-3.72338490e+00, -5.41533086e+00,  3.20996479e+00,\n        -1.88697595e+00,  3.84873419e+00, -2.78646049e+00,\n        -6.46468554e+00,  1.29651475e+01,  2.10774570e+00],\n       [ 5.26838360e-01,  8.85434106e-01,  3.75967732e-01,\n         4.79781327e+00,  5.80757207e+00,  2.71686971e+00,\n         3.98287384e+00,  9.16993351e-01,  3.36370464e+00],\n       [ 2.38777146e+00,  1.10474113e+00,  3.54622158e+00,\n         2.87238992e+00,  2.96308120e+00,  2.33607751e+00,\n         4.20931896e-01,  1.78739347e-01,  9.87722390e+00],\n       [ 2.49823571e+01,  2.50000000e+01,  2.49030026e+01,\n         2.49804741e+01,  2.49292153e+01,  2.48651528e+01,\n         2.49991866e+01,  2.49391928e+01,  2.50000000e+01],\n       [-9.18503504e-01, -1.34511016e-01,  2.63682116e+00,\n        -3.01513575e+00,  4.21480593e+00, -1.84516553e+00,\n        -3.77765073e+00,  3.47901996e+00,  1.31291715e+00],\n       [ 6.50454008e-01, -3.24568038e+00,  1.65936331e+00,\n         2.20957819e+00,  6.73383098e-01,  2.14991832e+00,\n        -2.57659773e+00, -4.62548226e-01, -5.58039238e-01],\n       [-3.03555234e+00, -2.16514766e+00, -3.57485720e+00,\n        -7.59063494e-01,  4.94606335e+00,  4.22162206e-01,\n        -3.52681790e+00,  5.83845793e+00,  3.67205361e+00],\n       [ 1.13069705e+00, -4.37423630e+00,  4.40097543e+00,\n        -3.38486172e+00, -4.27107386e-01,  4.80947924e+00,\n        -1.56993540e+00,  1.28231008e+00, -9.84795075e-01],\n       [-3.18322462e+00,  7.18347052e-01,  3.35700677e+00,\n        -2.16702440e+00,  3.40835622e+00, -4.21947813e-01,\n        -2.04123822e+00,  3.19639043e+00,  1.77427914e-01],\n       [-4.71984421e+00, -2.36992172e+00,  8.31044298e-01,\n        -3.12658270e+00,  4.10651189e+00,  1.70294950e+00,\n         3.50837144e-01,  1.35872797e+00,  3.78559562e+00],\n       [ 5.16922118e+00,  2.90254607e+00,  3.67591343e+00,\n        -3.06971366e+00, -2.54046677e+00,  2.26349473e+00,\n        -3.66717683e+00,  9.10130771e-01, -1.50419780e+00],\n       [ 5.59648683e+00,  9.77084742e-02,  3.26461308e+00,\n         5.17711643e+00,  8.78664991e-01,  3.50626931e+00,\n         3.32031088e-01,  7.85784972e-01,  3.96923276e+00]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('neurons_weights/semi_supervised_learning_e1.pickle', 'rb') as handle:\n",
    "    (weights, labels) = pickle.load(handle)\n",
    "\n",
    "weights = np.array(list(weights.values()))\n",
    "weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[25.861123696719716, 0.203043416723395, 0.1655152802112303,\n        0.5991356136936943, 0.2413446185999164, 0.6406430138787367,\n        23.198200574360683, 0.9817688508695817, 1.4643263594209617,\n        0.4344191223617231, 32.32226138326908, 1.3776481706985848,\n        0.7702141795723927, 2.572388896416791, 1.3727265936473012,\n        1.8547544570597458, 23.37406707865432, 25.687177938062625,\n        224.59858133090498, 1.9525976681953603, 0.4998313558518762,\n        1.8172984946047004, 0.8825259120714487, 3.044093334309192,\n        1.9193177921159355, 4.139751109569765, 23.607907950367984],\n       [None, 'bottle1', 'bottle1', 'bottle1', 'bottle1', 'bottle1',\n        None, 'bottle1', 'bottle1', 'bottle1', 'buzzer', 'bottle1',\n        'bottle1', 'bottle1', 'bottle1', 'bottle1', None, None, 'phone4',\n        'bottle1', 'bottle1', 'bottle1', 'bottle1', 'bottle1', 'bottle1',\n        'bottle1', None]], dtype=object)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.sum(weights, axis=1),\n",
    "          list(labels.values())])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "      label  phone4  buzzer  bottle1 predicted_label  success\n0    buzzer  1416.0    84.0     27.0          phone4    False\n1    phone4  4709.0    72.0     30.0          phone4     True\n2   bottle1   339.0    15.0      9.0          phone4    False\n3    phone4  6072.0    55.0     20.0          phone4     True\n4    buzzer  2985.0    46.0     27.0          phone4    False\n5    buzzer  2962.0    50.0     21.0          phone4    False\n6    buzzer  2252.0    64.0     24.0          phone4    False\n7   bottle1   347.0   114.0     34.0          phone4    False\n8    phone4  5446.0    12.0     14.0          phone4     True\n9   bottle1   354.0    10.0      4.0          phone4    False\n10  bottle1   243.0    11.0      7.0          phone4    False\n11   buzzer  2670.0    29.0     24.0          phone4    False\n12   buzzer  2259.0    30.0     16.0          phone4    False\n13  bottle1   326.0   102.0     30.0          phone4    False\n14  bottle1   297.0     9.0      5.0          phone4    False\n15   phone4  3010.0    12.0      4.0          phone4     True\n16   phone4  2354.0     8.0      3.0          phone4     True\n17   phone4  2578.0     7.0      3.0          phone4     True\n18   phone4  2209.0    21.0     12.0          phone4     True\n19   phone4  1507.0     8.0      3.0          phone4     True\n20   phone4  2433.0     5.0      2.0          phone4     True\n21   buzzer   829.0    28.0     15.0          phone4    False\n22   buzzer   836.0    26.0      2.0          phone4    False\n23   buzzer   482.0    27.0     20.0          phone4    False\n24   phone4  3082.0    21.0     20.0          phone4     True\n25   buzzer   531.0     6.0      4.0          phone4    False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>phone4</th>\n      <th>buzzer</th>\n      <th>bottle1</th>\n      <th>predicted_label</th>\n      <th>success</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>buzzer</td>\n      <td>1416.0</td>\n      <td>84.0</td>\n      <td>27.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>phone4</td>\n      <td>4709.0</td>\n      <td>72.0</td>\n      <td>30.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bottle1</td>\n      <td>339.0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>phone4</td>\n      <td>6072.0</td>\n      <td>55.0</td>\n      <td>20.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>buzzer</td>\n      <td>2985.0</td>\n      <td>46.0</td>\n      <td>27.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>buzzer</td>\n      <td>2962.0</td>\n      <td>50.0</td>\n      <td>21.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>buzzer</td>\n      <td>2252.0</td>\n      <td>64.0</td>\n      <td>24.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bottle1</td>\n      <td>347.0</td>\n      <td>114.0</td>\n      <td>34.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>phone4</td>\n      <td>5446.0</td>\n      <td>12.0</td>\n      <td>14.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>bottle1</td>\n      <td>354.0</td>\n      <td>10.0</td>\n      <td>4.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>bottle1</td>\n      <td>243.0</td>\n      <td>11.0</td>\n      <td>7.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>buzzer</td>\n      <td>2670.0</td>\n      <td>29.0</td>\n      <td>24.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>buzzer</td>\n      <td>2259.0</td>\n      <td>30.0</td>\n      <td>16.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>bottle1</td>\n      <td>326.0</td>\n      <td>102.0</td>\n      <td>30.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>bottle1</td>\n      <td>297.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>phone4</td>\n      <td>3010.0</td>\n      <td>12.0</td>\n      <td>4.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>phone4</td>\n      <td>2354.0</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>phone4</td>\n      <td>2578.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>phone4</td>\n      <td>2209.0</td>\n      <td>21.0</td>\n      <td>12.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>phone4</td>\n      <td>1507.0</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>phone4</td>\n      <td>2433.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>buzzer</td>\n      <td>829.0</td>\n      <td>28.0</td>\n      <td>15.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>buzzer</td>\n      <td>836.0</td>\n      <td>26.0</td>\n      <td>2.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>buzzer</td>\n      <td>482.0</td>\n      <td>27.0</td>\n      <td>20.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>phone4</td>\n      <td>3082.0</td>\n      <td>21.0</td>\n      <td>20.0</td>\n      <td>phone4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>buzzer</td>\n      <td>531.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>phone4</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['phone4', 'buzzer', 'bottle1']\n",
    "columns_labels = {\n",
    "    label: [c for c in res.columns if c.startswith(label)]\n",
    "    for label in labels\n",
    "  }\n",
    "\n",
    "\n",
    "sum_spikes_df = pd.DataFrame()\n",
    "sum_spikes_df['label'] = res['label']\n",
    "for label, columns in columns_labels.items():\n",
    "  sum_spikes_df[f'{label}'] = res[columns].sum(axis=1)\n",
    "sum_spikes_df['predicted_label'] = sum_spikes_df[labels].idxmax(axis=1)\n",
    "sum_spikes_df['success'] = sum_spikes_df['predicted_label'] == sum_spikes_df['label']\n",
    "sum_spikes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}